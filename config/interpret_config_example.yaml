# Example Configuration for epibench interpret

# --- Model and Data --- #

# Path to the configuration file used during the *training* of the model.
# This is needed to reconstruct the model architecture and potentially data loading parameters.
# Note: The interpret command also requires --checkpoint and --input-data via CLI.
training_config: path/to/training_output/train_config.yaml 

# Optional: Override data loading parameters if needed for interpretation.
# If not specified, parameters from the training_config's 'data' section might be used.
# data:
#   batch_size: 32 # Use a smaller batch size for interpretation if memory is limited
#   num_workers: 0

# --- Interpretation Parameters --- #
interpretation:
  # Method to use (currently only 'IntegratedGradients' is supported via the CLI).
  method: IntegratedGradients 
  
  # Parameters specific to Integrated Gradients.
  integrated_gradients:
    # Number of steps for the approximation integral.
    n_steps: 50
    
    # Baseline for comparison. Options:
    # - "zero": A tensor of zeros with the same shape as the input.
    # - "mean": (Not yet implemented in CLI) Calculate the mean across the dataset.
    # - path/to/baseline.h5: (Not yet implemented in CLI) Load a baseline from a file.
    baseline: "zero"
    
    # Index of the target output neuron to attribute for. 
    # For single-output regression models, this is typically 0.
    target_output_index: 0

  # --- Feature Extraction Parameters --- #
  # Parameters for identifying important features from attributions.
  feature_extraction:
    # Calculate attributions based on their absolute values.
    use_absolute_value: true
    
    # Option 1: Extract top K features based on attribution score.
    # top_k: 100 # Extract the indices of the top 100 features
    
    # Option 2: Extract features with attribution score above a threshold.
    # If both top_k and threshold are specified, top_k takes precedence.
    threshold: 0.1 # Extract indices where absolute attribution > 0.1

# --- Output Parameters --- #
output:
  # Whether to save the raw attribution scores (can be large).
  save_attributions: true # Saves attributions to a file like 'attributions.h5' or '.npy'
  
  # Whether to generate and save visualizations (e.g., attribution heatmaps).
  # Actual plot generation might depend on specific utilities available.
  generate_plots: true
  
  # Prefix for output filenames (e.g., 'interpretation_results_').
  # The CLI -o/--output-dir determines the directory.
  filename_prefix: "interpretation" 